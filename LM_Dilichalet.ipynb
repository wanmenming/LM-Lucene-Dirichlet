{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set query and parameter u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import math\n",
    "u = 1500.0\n",
    "\n",
    "query = [\"carbon emission\", \n",
    "            \"cutting carbon emission\", \n",
    "            \"flexible dieting\", \n",
    "            \"flexible dieting and vegetarianism\", \n",
    "            \"information of pest control in greenhouse\", \n",
    "            \"pest control greenhouse\", \n",
    "            \"greenhouse apples apple\", \n",
    "            \"green house apples apple\"]\n",
    "query_list = []\n",
    "for item in query:\n",
    "    query_list.append(item.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get corpus statistics: corpus size and token frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(),'corpus', '*.txt' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "i=0\n",
    "for file_path in file_list:\n",
    "    with open(file_path, encoding= 'utf-8') as file_input:\n",
    "           #created a dictionary of corpus with filenames as keys and content as value\n",
    "        #corpus[filenames[i]]= (file_input.read())\n",
    "        corpus[os.path.basename(file_path)]= (file_input.read())\n",
    "        #print(corpus[os.path.basename(file_path)])\n",
    "        #i +=1\n",
    "#print(type(corpus))\n",
    "#print(corpus[\"Observations_Concerning_the_Increase_of_Mankind_Peopling_of_Countries_etc..txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the frequency of word in corpus\n",
    "wordcount = defaultdict(int)\n",
    "for key,value in corpus.items():\n",
    "    for words in value.split():\n",
    "        if words not in wordcount:\n",
    "            wordcount[words] = 1\n",
    "        else:\n",
    "            wordcount[words] += 1\n",
    "#print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_size = sum(wordcount.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the LM_Dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carbon', 'emission']\n",
      "carbon\n",
      "emission\n",
      "['cutting', 'carbon', 'emission']\n",
      "cutting\n",
      "carbon\n",
      "emission\n",
      "['flexible', 'dieting']\n",
      "flexible\n",
      "dieting\n",
      "['flexible', 'dieting', 'and', 'vegetarianism']\n",
      "flexible\n",
      "dieting\n",
      "and\n",
      "vegetarianism\n",
      "['information', 'of', 'pest', 'control', 'in', 'greenhouse']\n",
      "information\n",
      "of\n",
      "pest\n",
      "control\n",
      "in\n",
      "greenhouse\n",
      "['pest', 'control', 'greenhouse']\n",
      "pest\n",
      "control\n",
      "greenhouse\n",
      "['greenhouse', 'apples', 'apple']\n",
      "greenhouse\n",
      "apples\n",
      "apple\n",
      "['green', 'house', 'apples', 'apple']\n",
      "green\n",
      "house\n",
      "apples\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rank_score = {}\n",
    "query_score = {}\n",
    "query_frequency = {}\n",
    "doc_size = {}\n",
    "query_lm = {}\n",
    "num = 0\n",
    "\n",
    "for i in query_list:\n",
    "    print(i)\n",
    "    query_relative_doc = []\n",
    "    doc_site_each_query = []\n",
    "    for query_item in i:\n",
    "        print(query_item)\n",
    "        find_docID = False\n",
    "        with open(\"unigramIDF.txt\", \"r\") as mytxt:\n",
    "            for lines in mytxt:      \n",
    "                key,value = lines.split(' : ')\n",
    "                if key == query_item: \n",
    "                    find_docID = True\n",
    "                    for doc_pattern in re.findall(r'\\((.[^\\)\\']+\\.txt,\\d+)\\)', value):\n",
    "                        query_relative_doc.append(doc_pattern)               \n",
    "                    \n",
    "                    for docId_frequency in query_relative_doc:\n",
    "                        #print(docId_frequency)\n",
    "                        mylist = docId_frequency.split(\",\")\n",
    "                        #print(mylist)\n",
    "                        #query_frequency[\"query_item\"] = query_item\n",
    "                        query_frequency[mylist[0]] = mylist[1]\n",
    "                        doc_site_each_query.append(mylist[0])\n",
    "                        if mylist[0] not in doc_size.keys():\n",
    "                            doc_size[mylist[0]] = len(corpus[mylist[0]].split())\n",
    "                    query_score[query_item] = query_frequency\n",
    "        if find_docID == False:\n",
    "            query_score[query_item] = \"Empty\"  \n",
    "    f_score = {}\n",
    "\n",
    "    for sdoc in doc_site_each_query:\n",
    "        LM_score = 0\n",
    "        for squery in i:\n",
    "            #print(doc)\n",
    "            if wordcount[squery] != 0:\n",
    "                if query_score[squery] != \"Empty\":\n",
    "                #print(query_score[squery])\n",
    "                    if sdoc not in query_score[squery].keys():\n",
    "                        f_score[sdoc] = 0\n",
    "                    else:\n",
    "                        f_score[sdoc] = query_score[squery][sdoc]\n",
    "\n",
    "                else:\n",
    "                    f_score[sdoc] = 0\n",
    "                #print(f_score[sdoc],u * (wordcount[squery]/corpus_size),doc_size[sdoc])\n",
    "                q_score = math.log((int(f_score[sdoc]) + u * (wordcount[squery]/corpus_size))/(doc_size[sdoc] + u))\n",
    "            else:\n",
    "                q_score = 0\n",
    "            LM_score += q_score\n",
    "            #print(LM_score)\n",
    "        rank_score[sdoc] =  LM_score\n",
    "        \n",
    "  \n",
    "    sorted_by_value = sorted(rank_score.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    \n",
    "    write_path = os.getcwd() + \"/task2\" \n",
    "    filename = os.path.join(write_path, query[num] + \".txt\")\n",
    "    f = open(filename,\"w+\",encoding=\"utf-8\",newline='\\n')\n",
    "    count = 1 \n",
    "    for top_100 in sorted_by_value:\n",
    "        if count < 101:\n",
    "            f.write(\"%d %s %f\\n\" %(count,top_100[0],top_100[1]))\n",
    "            count = count+1\n",
    "        else:\n",
    "            break\n",
    "    f.close()\n",
    "    \n",
    "    f = open(\"top100Task2.txt\",\"a+\",encoding=\"utf-8\",newline='\\n')\n",
    "    count = 1 \n",
    "    #Format: query_id Q0 doc_id rank LMDirichlet_score system_name\n",
    "    constant = \"Q0\"\n",
    "    system_name = \"Unigram_LMDirichlet_MingmingLi\"\n",
    "    for top_100 in sorted_by_value:\n",
    "        if count < 101:\n",
    "            f.write(\"%d %s %s %s %f %s\\n\" %(num+1,constant,top_100[0],count,top_100[1],system_name))\n",
    "            count = count+1\n",
    "        else:\n",
    "            break\n",
    "    f.close() \n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#badk up function: get the frequency of query_word in specific document\n",
    "def get_frequency(query_word,docID):\n",
    "    #print(docID)\n",
    "    frequency = 0\n",
    "    doc_len = 0\n",
    "    #print(corpus[docID])\n",
    "    for word in corpus[docID].split():\n",
    "        doc_len += 1\n",
    "        if word == query_word:\n",
    "            frequency += 1\n",
    "            \n",
    "    return frequency,doc_len\n",
    "f,l = get_frequency(\"carbon\",\"2009_United_Nations_Climate_Change_Conference.txt\")\n",
    "#print(f)\n",
    "#print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
